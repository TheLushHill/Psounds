output_dir: "Model\\logs\\logs_s1"
train_semantic_path: "Model\\Training\\List_file\\list3\\6_name2semantic_0.tsv"
train_phoneme_path: "Model\\Training\\List_file\\list1\\2_name2text_0.txt"
pretrained_s1: Model/Training/pretrained_models/gsv-v2final-pretrained/s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt
train:
  seed: 1234
  epochs: 12
  batch_size: 2
  save_every_n_epoch: 4
  precision: 16-mixed
  gradient_clip: 1.0
  if_save_latest: true
  if_save_every_weights: true
  half_weights_save_dir: Model\\Training\\GPT_weights
optimizer:
  lr: 0.01
  lr_init: 0.00001
  lr_end: 0.0001
  warmup_steps: 2000
  decay_steps: 40000
data:
  max_eval_sample: 8
  max_sec: 54
  num_workers: 4
  pad_val: 1024 # same with EOS in model
model:
  vocab_size: 1025
  phoneme_vocab_size: 732
  embedding_dim: 512
  hidden_dim: 512
  head: 16
  linear_units: 2048
  n_layer: 24
  dropout: 0
  EOS: 1024
  random_bert: 0
inference:
  top_k: 15
